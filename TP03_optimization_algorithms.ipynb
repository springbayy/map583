{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kscaman/MAP583/blob/main/TP/TP03_optimization_algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5yGqm4Nl39f"
      },
      "source": [
        "# Optimization algorithms\n",
        "\n",
        "Code adapted from the [optimization chapter](http://www.d2l.ai/chapter_optimization/) of Dive into Deep Learning. The objectives are:\n",
        "\n",
        "1.   To show the advantages and disadvantages of several popular optimization algorithms on simple 2d functions.\n",
        "2.   To explore the impact of gradient noise and mini-batch size on the training of simple models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9hjpWQPwE3R"
      },
      "source": [
        "We will thus fix the function to learn, and try several optimization algorithms on it. The following functions are used to access the gradient (`get_gradient`), run an optimization loop with a given update rule (`optimize`), and show the trace of this optimization loop on a figure (`show_trace_2d`). `show_trace_2d` requires as inputs: the function to optimize and the output of the `optimize` function. The `update` rule performs one iteration of the optimization algorithm, and will use internally `get_gradient` to access and use the gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4gJMsVnYl39r"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_gradient(f, x, noise_std=0):\n",
        "    \"\"\"Returns the gradient of a function f at x with additive Gaussian noise.\"\"\"\n",
        "    x = x.detach() # Removes information about the gradient\n",
        "    x.requires_grad = True\n",
        "    output = f(x)\n",
        "    output.backward()\n",
        "    gradient = x.grad\n",
        "    noise = torch.randn(gradient.shape)\n",
        "    return gradient + noise_std * noise\n",
        "\n",
        "def optimize(update, num_iter=20, init_s=0):\n",
        "    \"\"\"Optimize the objective function of 2d variables with a customized update.\"\"\"\n",
        "    \"\"\"update(x,s_x) should return the updated positions x and possible memory terms.\"\"\"\n",
        "    x = torch.Tensor([-5, -2])\n",
        "    s = init_s\n",
        "    all_x = [x]\n",
        "    for i in range(num_iter):\n",
        "        x, s = update(x, s)\n",
        "        all_x.append((x))\n",
        "    print('epoch %d, x1 %f, x2 %f' % (i+1, x[0], x[1]))\n",
        "    return torch.stack(all_x, dim=0)\n",
        "\n",
        "def show_trace_2d(f, all_x, color='red', is_new_plot=True):\n",
        "    \"\"\"Show the trace of 2d variables during optimization.\"\"\"\n",
        "    # Plot the iterates x_t\n",
        "    if is_new_plot:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    else:\n",
        "        fig = plt.gcf()\n",
        "        ax1 = fig.axes[0]\n",
        "        ax2 = fig.axes[1]\n",
        "    ax1.plot(all_x[:,0], all_x[:,1], '-o', color=color)\n",
        "    all_f = np.array([f(torch.Tensor([x[0], x[1]])) for x in all_x])\n",
        "\n",
        "    # Plot the function's level sets\n",
        "    x1 = np.arange(-5.5, 3.5, 0.1)\n",
        "    x2 = np.arange(-3.0, 2.0, 0.1)\n",
        "    f_grid = np.array([[f(torch.Tensor([u, v])) for u in x1] for v in x2])\n",
        "    ax1.contour(x1, x2, f_grid, colors='blue')\n",
        "    ax1.set_xlim([-5.5, 3.5])\n",
        "    ax1.set_ylim([-3.0, 2.0])\n",
        "    ax1.set_xlabel('x1')\n",
        "    ax1.set_ylabel('x2')\n",
        "\n",
        "    # Plot the function values f(x_t)\n",
        "    ax2.plot(all_f, color=color)\n",
        "    ax2.set_xlabel('time ($t$)')\n",
        "    ax2.set_ylabel('function value ($f(x_t)$)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89zLfOSGZpsR"
      },
      "source": [
        "Write a function `f` that returns $f(x) = 0.1 x_0^2 + 2 x_1^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mp25f8OkqSwv"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    return 0.1*f[0]**2+2*f[1]**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlFPt-C9l3-N"
      },
      "source": [
        "## Gradient descent\n",
        "\n",
        "Write a function `gd(x, s)` that takes as input a point `x` and (unused) memory state `s`, and performs one iteration (update rule) of gradient descent. Remember that `get_gradient(f, x)` returns the gradient of `f` at `x`. Then, test your update rule with step sizes `eta` equal to $0.4$ and $0.6$ using `show_trace_2d(f, optimize(gd))`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UVnxAGegl3-R"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'detach'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m         x[i]\u001b[38;5;241m=\u001b[39mxi\u001b[38;5;241m-\u001b[39meta\u001b[38;5;241m*\u001b[39mgrad[i]\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m---> 13\u001b[0m \u001b[43mgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[8], line 5\u001b[0m, in \u001b[0;36mgd\u001b[1;34m(x, s)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgd\u001b[39m(x, s):\n\u001b[1;32m----> 5\u001b[0m     grad\u001b[38;5;241m=\u001b[39m\u001b[43mget_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[0;32m      8\u001b[0m         xi\u001b[38;5;241m=\u001b[39mx[i]\n",
            "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mget_gradient\u001b[1;34m(f, x, noise_std)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_gradient\u001b[39m(f, x, noise_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the gradient of a function f at x with additive Gaussian noise.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m() \u001b[38;5;66;03m# Removes information about the gradient\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     x\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     output \u001b[38;5;241m=\u001b[39m f(x)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
          ]
        }
      ],
      "source": [
        "eta = 0.4\n",
        "\n",
        "def gd(x, s):\n",
        "\n",
        "    grad=get_gradient(f, x)\n",
        "\n",
        "    for i in x:\n",
        "        xi=x[i]\n",
        "        x[i]=xi-eta*grad[i]\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcsKwAyRl3-y"
      },
      "source": [
        "## Momemtum\n",
        "\n",
        "Write a function `momentum(x, v)` that performs one iteration of the momentum method. Test your update rule on the same step-sizes as gradient descent. What happens? Try also with `eta, gamma = 0.05, 0.9`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qnyJQwnl3-0"
      },
      "outputs": [],
      "source": [
        "eta, gamma = 0.4, 0.5\n",
        "\n",
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WjZp2WPl3_T"
      },
      "source": [
        "## Nesterov accelerated gradient\n",
        "\n",
        "Now write the function `nesterov(x, v)` for Nesterov's accelerated grdient descent. Compare with momentum with the same hyper-parameters `eta, gamma = 0.05, 0.9`. You can use `is_new_plot=False` to show both plots on the same graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s02JrcZRl3_V"
      },
      "outputs": [],
      "source": [
        "eta, gamma = 0.05, 0.9\n",
        "\n",
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyzF7aAXl3_h"
      },
      "source": [
        "## Adagrad\n",
        "\n",
        "Same for `adagrad(x, s)` with $\\varepsilon = 10^{-6}$, and test it with `eta = 0.4` and `eta = 1.5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mk4sixgl3_i"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# YOUR CODE HERE (hint: use x**a for coordinate-wise power a)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtU9tLiml3_2"
      },
      "source": [
        "## RMSProp\n",
        "\n",
        "Same for `rmsprop(x, s)`, and test it with `eta, gamma = 0.4, 0.9`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBbRL5h3l3_3"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9ILdGVl4AD"
      },
      "source": [
        "## Adam\n",
        "\n",
        "Same for `adam(x, s)`, where the memory state will be a tuple `m, v, t = s` where `m` and `v` are the internal states, while `t` is the current iteration. Test it with `beta1, beta2, eta = 0.9, 0.99, 0.8`, and remember to initialize the internal state in the function `show_trace_2d`, here to `init_s=(0, 0, 0)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC3McRDZl4AF"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLmMd9h4l4Aa"
      },
      "source": [
        "## AMSGrad\n",
        "\n",
        "Same for `amsgrad(x, s)`, where the memory state will be a tuple `m, v, v_bar = s` (see slides for the meaning of each value). Compare Adam and AMSGrad with `beta1, beta2, eta = 0.9, 0.99, 1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khhsvs3Dl4Ac"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4egXGTwJKO1I"
      },
      "source": [
        "# Stochastic optimization and schedulers\n",
        "\n",
        "We will now see what happens when the gradient is noisy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6atNmeNwMmp"
      },
      "source": [
        "First, write a function `sgd(x, s)` that performs one step of stochastic gradient descent, by adding a noise of standard deviation $0.8$ to the gradient at each iteration using `get_gradient(f, x, noise_std=0.8)`. Compare the effect of different different step-sizes $\\eta\\in\\{0.4,0.2,0.05\\}$ on the convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fud-H4nKpmv"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylIFUmv0pCDW"
      },
      "source": [
        "A high learning rate is faster, but a small learning rate leads to better final performance.\n",
        "Propose a modification of sgd that use behavior this to its advantage in an update rule named `sgd_with_scheduler`. Compare its behavior with standard sgd in the previous setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKVlpBm0ovJY"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYurAPE5hcwf"
      },
      "source": [
        "We are now going to test sgd on a more realistic example: linear classification of a toy dataset. First, we generate a dataset with 2 classes, each being a distributed according to a Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDjhHB70tupL"
      },
      "outputs": [],
      "source": [
        "# Generating a dataset of two Gaussian classes and 100 samples\n",
        "N = 100\n",
        "v = torch.Tensor([1, 1]).unsqueeze(0)\n",
        "Y = 2 * torch.randint(low=0, high=2, size=(N, 1), dtype=torch.float) - 1\n",
        "X = 1 * torch.randn(N, 2) + Y @ v\n",
        "plt.plot(X[Y.flatten() > 0,0], X[Y.flatten() > 0,1], 'o', color='red')\n",
        "plt.plot(X[Y.flatten() < 0,0], X[Y.flatten() < 0,1], 'o', color='blue')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BUl91ppxOdF"
      },
      "source": [
        "Now, write `mse_linear(theta)` that returns the MSE loss over the toy dataset for a simple linear model $y = \\theta X$, as well as `mse_linear_mb(theta)` that returns the MSE loss over a random mini-batch of size `mini_batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mD11e7398P1"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# YOUR CODE HERE\n",
        "# WARNING: theta should be of size (2,1)\n",
        "#\n",
        "\n",
        "mini_batch_size = 1\n",
        "#\n",
        "# YOUR CODE HERE\n",
        "# WARNING: theta should be of size (2,1)\n",
        "# HINT: use torch.randperm\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXiUg6cEyITr"
      },
      "source": [
        "Now, test a mini-batch version of Adam on this toy dataset, with `eta=0.1` and `mini_batch_size` in $\\{1,5,10\\}$. What do you see?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm6m5pwqwVBU"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K72-R5GBtIAO"
      },
      "source": [
        "# Pytorch optimizers and schedulers\n",
        "Typical training loop, optimizers and schedulers in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG9AtGl4YF1X"
      },
      "outputs": [],
      "source": [
        "# MINIMAL (AND INCOMPLETE) EXAMPLE\n",
        "for epoch in range(20):\n",
        "    for inputs, targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDAB3a6utNtx"
      },
      "outputs": [],
      "source": [
        "# MORE DETAILED AND COMPLETE EXAMPLE\n",
        "\n",
        "### PARAMETERS\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 100\n",
        "\n",
        "### DATA, MODEL, LOSS, OPTIMIZER AND SCHEDULER\n",
        "dataloader = ... # YOUR DATA\n",
        "model = ... # YOUR MODEL\n",
        "criterion = ... # YOUR LOSS FUNCTION\n",
        "optimizer = ... # YOUR OPTIMIZER\n",
        "scheduler = ... # YOUR SCHEDULER\n",
        "\n",
        "### TRAINING LOOP\n",
        "# Prepares the model for training (needed for some models)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # One training epoch over the whole dataset\n",
        "    for inputs, targets in dataloader:\n",
        "        # One mini-batch, put on the desired devide (cpu or gpu)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Reinitialize the gradients before any computation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Computation of the model's output and loss on the mini-batch\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Computation of the gradient on the mini-batch\n",
        "        loss.backward()\n",
        "\n",
        "        # One iteration of the optimizer and update of the step-size\n",
        "        optimizer.step()\n",
        "\n",
        "        # Then we can compute statistics and store loss values\n",
        "        ...\n",
        "\n",
        "    # Update of the step-size\n",
        "    scheduler.step()\n",
        "    print('Loss: {:.4f} Acc: {:.4f}'.format(..., ...))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71m6hmkoTLfh"
      },
      "outputs": [],
      "source": [
        "# Typical optimizers and schedulers\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: 1/t**0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv4ZaNF4X2Xa"
      },
      "outputs": [],
      "source": [
        "# If you want to implement your own optimizer\n",
        "class SuperDuperOptimizer(optim.Optimizer):\n",
        "    def __init__(self, params, lr):\n",
        "        defaults = dict(lr=lr)\n",
        "        super(SuperDuperOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    def step():\n",
        "        ..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "K72-R5GBtIAO"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
